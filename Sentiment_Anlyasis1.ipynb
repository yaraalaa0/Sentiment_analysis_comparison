{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yara'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "x = stemmer.stem('Yara')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intelligence'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatizer.lemmatize('Intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Unnamed: 0                                             review  \\\n",
       "0               0  I went and saw this movie last night after bei...   \n",
       "1               1  Actor turned director Bill Paxton follows up h...   \n",
       "2               2  As a recreational golfer with some knowledge o...   \n",
       "3               3  I saw this film in a sneak preview, and it is ...   \n",
       "4               4  Bill Paxton has taken the true story of the 19...   \n",
       "5               5  I saw this film on September 1st, 2005 in Indi...   \n",
       "6               6  Maybe I'm reading into this too much, but I wo...   \n",
       "7               7  I felt this film did have many good qualities....   \n",
       "8               8  This movie is amazing because the fact that th...   \n",
       "9               9  \"Quitting\" may be as much about exiting a pre-...   \n",
       "10             10  I loved this movie from beginning to end.I am ...   \n",
       "11             11  I was fortunate to attend the London premier o...   \n",
       "12             12  I first saw this movie on IFC. Which is a grea...   \n",
       "13             13  I must say, every time I see this movie, I am ...   \n",
       "14             14  My wife is a mental health therapist and we wa...   \n",
       "15             15  I saw this film at the Rotterdam International...   \n",
       "16             16  \"Night of the Hunted\" stars French porn star B...   \n",
       "17             17  Even if you're a fan of Jean Rollin's idiosync...   \n",
       "18             18  I was surprised how much I enjoyed this. Sure ...   \n",
       "19             19  I went into \"Night of the Hunted\" not knowing ...   \n",
       "20             20  I have certainly not seen all of Jean Rollin's...   \n",
       "21             21  Since this cartoon was made in the old days, F...   \n",
       "22             22  Despite the title and unlike some other storie...   \n",
       "23             23  Felix in Hollywood is a great film. The versio...   \n",
       "24             24  A gem of a cartoon from the silent era---it wa...   \n",
       "25             25  This short is one of the best of all time and ...   \n",
       "26             26  Felix is watching an actor rehearse his lines:...   \n",
       "27             27  While I can't say whether or not Larry Hama ev...   \n",
       "28             28  Errol Flynn's roguish charm really shines thro...   \n",
       "29             29  Warner Brothers tampered considerably with Ame...   \n",
       "...           ...                                                ...   \n",
       "49970       49970  I remember watching this movie several times a...   \n",
       "49971       49971  The first scene in 'Problem Child' has a baby ...   \n",
       "49972       49972  This kid is rather bad, but in no way do they ...   \n",
       "49973       49973  My girlfriend and I were stunned by how bad th...   \n",
       "49974       49974  This is one of the worst movies I have ever se...   \n",
       "49975       49975  Having just recently re-viewed \"Lipstick\" for ...   \n",
       "49976       49976  Don't waste 90 minutes of your time on \"Fast F...   \n",
       "49977       49977  This movie was billed as a comedy and a myster...   \n",
       "49978       49978  Story starts slow and nothing funny happens fo...   \n",
       "49979       49979  This film is a massive Yawn proving that Ameri...   \n",
       "49980       49980  It was a Sunday night and I was waiting for th...   \n",
       "49981       49981  Have I ever seen a film more shockingly inept?...   \n",
       "49982       49982  The next time you are at a party and someone a...   \n",
       "49983       49983  Turgid dialogue, feeble characterization - Har...   \n",
       "49984       49984  Cameron Diaz is a woman who is married to a ju...   \n",
       "49985       49985  I had the misfortune to watch this rubbish on ...   \n",
       "49986       49986  It's pretty bad when the generic movie synopsi...   \n",
       "49987       49987  Having watched this movie on the SciFi channel...   \n",
       "49988       49988  First off, I'm not here to dog this movie. I f...   \n",
       "49989       49989  Ah yez, the Sci Fi Channel produces Yeti anoth...   \n",
       "49990       49990  Yeti: Curse of the Snow Demon starts aboard a ...   \n",
       "49991       49991  Hmmm, a sports team is in a plane crash, gets ...   \n",
       "49992       49992  I saw this piece of garbage on AMC last night,...   \n",
       "49993       49993  Although the production and Jerry Jameson's di...   \n",
       "49994       49994  Capt. Gallagher (Lemmon) and flight attendant ...   \n",
       "49995       49995  Towards the end of the movie, I felt it was to...   \n",
       "49996       49996  This is the kind of movie that my enemies cont...   \n",
       "49997       49997  I saw 'Descent' last night at the Stockholm Fi...   \n",
       "49998       49998  Some films that you pick up for a pound turn o...   \n",
       "49999       49999  This is one of the dumbest films, I've ever se...   \n",
       "\n",
       "       sentiment  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "5              1  \n",
       "6              1  \n",
       "7              1  \n",
       "8              1  \n",
       "9              1  \n",
       "10             1  \n",
       "11             1  \n",
       "12             1  \n",
       "13             1  \n",
       "14             1  \n",
       "15             1  \n",
       "16             1  \n",
       "17             1  \n",
       "18             1  \n",
       "19             1  \n",
       "20             1  \n",
       "21             1  \n",
       "22             1  \n",
       "23             1  \n",
       "24             1  \n",
       "25             1  \n",
       "26             1  \n",
       "27             1  \n",
       "28             1  \n",
       "29             1  \n",
       "...          ...  \n",
       "49970          0  \n",
       "49971          0  \n",
       "49972          0  \n",
       "49973          0  \n",
       "49974          0  \n",
       "49975          0  \n",
       "49976          0  \n",
       "49977          0  \n",
       "49978          0  \n",
       "49979          0  \n",
       "49980          0  \n",
       "49981          0  \n",
       "49982          0  \n",
       "49983          0  \n",
       "49984          0  \n",
       "49985          0  \n",
       "49986          0  \n",
       "49987          0  \n",
       "49988          0  \n",
       "49989          0  \n",
       "49990          0  \n",
       "49991          0  \n",
       "49992          0  \n",
       "49993          0  \n",
       "49994          0  \n",
       "49995          0  \n",
       "49996          0  \n",
       "49997          0  \n",
       "49998          0  \n",
       "49999          0  \n",
       "\n",
       "[50000 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_data.csv\")\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_process(raw_review):\n",
    "    # Remove punctuations and html tags\n",
    "    # Beautiful soup must be applied before this function is called\n",
    "    sent = raw_review.get_text()\n",
    "    words = word_tokenize(sent)\n",
    "    punctuationFiltered= [w.lower() for w in words if w.isalpha()]\n",
    "    shortwordsFiltered= [w for w in punctuationFiltered if len(w) >= 3]\n",
    "    return( ' '.join(shortwordsFiltered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "def remove_stopwords(raw_review):\n",
    "    # Remove stop words\n",
    "    words = word_tokenize(raw_review)\n",
    "    stopwordsFiltered= [w for w in words if w not in stopWords]\n",
    "    return( ' '.join(stopwordsFiltered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SnowballStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-57cc0ceca867>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreview_stemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_review\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Stemming function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_review\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstemming_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SnowballStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def review_stemming(raw_review):\n",
    "    # Stemming function\n",
    "    words = raw_review.split()\n",
    "    stemming_words = [stemmer.stem(w) for w in words]\n",
    "    return( ' '.join(stemming_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "def review_lemmatize(raw_review):\n",
    "    words = raw_review.split()\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return( ' '.join(lemma_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py:3194: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 3194 of the file E:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  mapped = lib.map_infer(values, f, convert=convert_dtype)\n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].apply(BeautifulSoup)\n",
    "data['review'] = data['review'].apply(review_process)\n",
    "data['review'] = data['review'].apply(remove_stopwords)\n",
    "data['review'] = data['review'].apply(review_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(X_train)\n",
    "x_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200, max_depth = 7, random_state=0)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Baseline Model Score: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8369"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The Baseline Model Score: \")\n",
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"movie_data.csv\")\n",
    "data3 = data2\n",
    "data4 = data2\n",
    "data5 = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py:3194: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 3194 of the file E:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  mapped = lib.map_infer(values, f, convert=convert_dtype)\n"
     ]
    }
   ],
   "source": [
    "data2['review'] = data2['review'].apply(BeautifulSoup)\n",
    "data2['review'] = data2['review'].apply(review_process)\n",
    "data2['review'] = data2['review'].apply(remove_stopwords)\n",
    "data2['review'] = data2['review'].apply(review_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(data2['review'], data2['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x_train2 = vectorizer.fit_transform(X_train2)\n",
    "x_test2 = vectorizer.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200, max_depth = 7, random_state=0)\n",
    "clf.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lemmatizing Model Score: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8309"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The Lemmatizing Model Score: \")\n",
    "clf.score(x_test2,y_test2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Comment: Using lemmatization gets lower score than using stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py:3194: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 3194 of the file E:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  mapped = lib.map_infer(values, f, convert=convert_dtype)\n"
     ]
    }
   ],
   "source": [
    "data3['review'] = data3['review'].apply(BeautifulSoup)\n",
    "data3['review'] = data3['review'].apply(review_process)\n",
    "data3['review'] = data3['review'].apply(review_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The No Removal of Stopwords Model Score: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8369"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(data3['review'], data3['sentiment'], test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train3 = vectorizer.fit_transform(X_train3)\n",
    "x_test3 = vectorizer.transform(X_test3)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth = 7, random_state=0)\n",
    "clf.fit(x_train3, y_train3)\n",
    "print(\"The No Removal of Stopwords Model Score: \")\n",
    "clf.score(x_test3,y_test3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Comment: No difference in accuracy from baseline model\n",
    "## The removal of stop words is not significant in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Count_Vectorizer Model Score: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8386"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train = vectorizer.fit_transform(X_train)\n",
    "x_test = vectorizer.transform(X_test)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth = 7, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"The Count_Vectorizer Model Score: \")\n",
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Comment: Using count vectorizer gets slightly higher score than tfidf vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding NOT_ to all words after negative words and verbs till the end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = set([\"not\", \"no\", \"don't\", \"doesn't\", \"didn't\", \"hasn't\", \"haven't\", \"hadn't\", \"can't\", \"couldn't\", \"shouldn't\" ])\n",
    "sentence_end = False\n",
    "def adding_NOT(raw_review):\n",
    "    # Adding not to the words after negative words. \n",
    "    # Append \"NOT_\" till the sentence end if sentence_end == True\n",
    "    # else append \"NOT_\" to only 3 words after.\n",
    "    Modified_words = []\n",
    "    sents = sent_tokenize(raw_review)\n",
    "    for sent in sents:\n",
    "        words = word_tokenize(sent)\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            #print(len(words))\n",
    "            if words[i] not in negative_words:\n",
    "                Modified_words.append(words[i])\n",
    "            else:\n",
    "                Modified_words.append(words[i])\n",
    "                if sentence_end == True:\n",
    "                    while i != len(words):\n",
    "                        Modified_words.append('NOT_'+words[i])\n",
    "                        i += 1\n",
    "                    break\n",
    "                else:\n",
    "                    if (len(words) -i -1) < 3:\n",
    "                        limit = len(words) -i -1\n",
    "                    else:\n",
    "                        limit = 3\n",
    "                    for x in range(limit):\n",
    "                        i = i + 1\n",
    "                        Modified_words.append('NOT_'+words[i])\n",
    "            i += 1\n",
    "                    \n",
    "    return( ' '.join(Modified_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_end = True\n",
    "data4['review'] = data['review'].apply(adding_NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Adding NOT_ till end Model Score: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8331"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(data4['review'], data4['sentiment'], test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train4 = vectorizer.fit_transform(X_train4)\n",
    "x_test4 = vectorizer.transform(X_test4)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth = 7, random_state=0)\n",
    "clf.fit(x_train4, y_train4)\n",
    "print(\"The Adding NOT_ till end Model Score: \")\n",
    "clf.score(x_test4,y_test4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Comment: The accuracy has decreased!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding NOT_ to only three words after the negative word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_end = False\n",
    "data5['review'] = data['review'].apply(adding_NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Adding NOT_ to 3 Words Model Score: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8323"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(data5['review'], data5['sentiment'], test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train5 = vectorizer.fit_transform(X_train5)\n",
    "x_test5 = vectorizer.transform(X_test5)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth = 7, random_state=0)\n",
    "clf.fit(x_train5, y_train5)\n",
    "print(\"The Adding NOT_ to 3 Words Model Score: \")\n",
    "clf.score(x_test5,y_test5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Comment: The accuracy has decreased also! \n",
    "## That means adding NOT_ didn't improve the accuracy of the baseline model in either cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8a97a6b14fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Learning\\DL\\Miniconda\\envs\\tensorflow\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "x_train = x_train.toarray()\n",
    "x_test = x_test.toarray()\n",
    "gnb.fit(x_train, y_train)\n",
    "gnb.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## I keep getting memory error because the sparse matrix size is huge and Naive Bayes requires transforimng it into an array. \n",
    "## Maybe if you run it on your laptop it won't run out of memory. :''D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
